{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0befe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from common_utils.utils.config import Config\n",
    "from common_utils.io.data_access.data_access_factory import DataAccessFactory\n",
    "\n",
    "# from axpo_trading.forecast.forecast_preprocess_iberia import preproces_ufis\n",
    "from common_utils.utils import utils, utils_io, utils_date\n",
    "from axpo_trading.forecast import forecast_sql_preprocess_iberia\n",
    "from axpo_trading.forecast import forecast_preprocess_iberia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random seeds\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(42)\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "set_random_seed(42)\n",
    "import random as rn\n",
    "\n",
    "rn.seed(1254)\n",
    "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# wind_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "wind_path = \"/home/jovyan/projects/AdvancedAnalytics-UseCase-Wind\"\n",
    "os.chdir(wind_path)\n",
    "\n",
    "os.environ[\"CONFIG_DIR\"] = \"config_files\"\n",
    "os.environ[\"AUTH_CONFIG_DIR\"] = \"auth\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_CONTAINER_NAME_WIND_RAW\"] = \"raw\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_CONTAINER_NAME_WIND_STAGING\"] = \"staging\"\n",
    "os.environ[\n",
    "    \"AZURE_SQL_SHARED_RAW_SERVER\"\n",
    "] = \"axso-prod-appl-aa-prod-shared-sql-secondary.database.windows.net\"\n",
    "# os.environ[\"AZURE_SQL_SHARED_RAW_SERVER\"] = 'axso-prod-appl-aa-prod-shared-sql.database.windows.net'\n",
    "os.environ[\"AZURE_SQL_SHARED_RAW_DATABASE\"] = \"axso-prod-appl-aa-prod-shared-raw-sqldb\"\n",
    "os.environ[\"N_THREADS_SQL\"] = \"1\"\n",
    "\n",
    "# DEV\n",
    "os.environ[\"ENV\"] = \"azure_iberia_k8s_dev\"\n",
    "# BLOB DEV\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_NAME\"] = \"axsonpaadevdslabdls\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_NAME\"] = \"axsoprodaaprodshareddls-secondary\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_CONTAINER_NAME_WIND_REFINED\"] = \"wind-refined\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_CONTAINER_NAME_WIND_RESULTS\"] = \"wind-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c2ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from_train = \"2021-05-01\"\n",
    "date_to_train = \"2022-03-01\"\n",
    "# date_from_train = \"2021-01-01\"\n",
    "# date_to_train = \"2021-05-20\"\n",
    "\n",
    "date_from_validate = \"2022-03-02\"\n",
    "date_to_validate = \"2022-05-01\"\n",
    "\n",
    "date_from_test = \"2022-05-01\"\n",
    "date_to_test = \"2022-06-01\"\n",
    "\n",
    "\n",
    "n_steps_in = 12\n",
    "n_steps_out = 3\n",
    "\n",
    "portfolio_level = True\n",
    "\n",
    "main_premaster_columns = [\"datetime_market\",\"datetime\",\"hours_fwd\",\"ufi\",\"telemetry\",\"forecast\",\"metering\"]\n",
    "info_columns = [\"telemetry\",\"forecast\",\"metering\",\"forecast_error_metering\",\"forecast_error_telemetry\"]\n",
    "groupping_columns = ['ufi','hours_fwd']\n",
    "target_hours_fwd = [1,2,3]\n",
    "# target_ufis = [\"ZAPATER\",\"PEARBO\",\"ROMERA\"]\n",
    "# target_ufis = [\"ABELLA\",\"PAXAMON\",\"SPADRON\",\"PELALIN\",\"TIGUEIR\",\"PEIRIXO\",\"MONTCEO\",\"MONTOUT\"]\n",
    "target_ufis= ['ABELLA', 'CERROS', 'LAMESA', 'LACAYA', 'VILACHA', 'TIGUEIR',\n",
    "       'ESQUILE', 'BRULLES', 'PELALIN', 'PESLOB', 'VISOS', 'DEFERII',\n",
    "       'PECORTI', 'LASORDA', 'ESCANDO', 'BAYO', 'HINOJII',\n",
    "       'PEOCHAO', 'CALERA', 'CPELAOS', 'ELGALLO', 'SPADRON', 'PAXAMON',\n",
    "       'TRAPERA', 'SABUCED', 'PEZARZU', 'PESLOA', 'ASNEVES', 'CAMPANA',\n",
    "       'PECOUTE', 'HINOJAI', 'PESLOD', 'AXIABRE', 'FEIXOS', 'OTERO',\n",
    "       'POTRA', 'ZARZUEL', 'CERCEDA', 'GRAIADE', 'PEOUROL', 'RODERA',\n",
    "       'MONTOUT', 'ARTEIXO', 'ELLLAN', 'MONTCEO', 'LALOMBA', 'CARRACE',\n",
    "       'PEIRIXO', 'ATALAYA', 'FRAILA', 'DEHESII', 'MONTERO', 'MONDONE',\n",
    "       'ROMERA', 'ESE', 'BANDELE', 'SANJOSE', 'SERRETA', 'DEHEII',\n",
    "       'AEROGEN', 'ZAPATER', 'LARUYA', 'PESLOC', 'PEARBO', 'PELALOM',\n",
    "       'MUDEFER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a660e",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a3b40",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea59cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_master_by_levels(multiple_line_df):\n",
    "\n",
    "    # TODO Parametrize levels\n",
    "\n",
    "    # Pivot data according to level\n",
    "    # Level 1: market dependent columns\n",
    "    index_cols = [\"datetime_market\"]\n",
    "    market_level_columns = [\"hour_market\"]\n",
    "    reduced_df_lv_1 = multiple_line_df[market_level_columns + index_cols]\n",
    "    reduced_df_lv_1[\"day_market\"] = multiple_line_df[\"datetime_market\"].dt.day\n",
    "    reduced_df_lv_1[\"month_market\"] = multiple_line_df[\"datetime_market\"].dt.month\n",
    "    # Add missing levels to even the final shapes\n",
    "    reduced_df_lv_1 = reduced_df_lv_1.drop_duplicates()\n",
    "    reduced_df_lv_1 = reduced_df_lv_1.set_index(index_cols, drop=True)\n",
    "    even_level_1_arrays = [\n",
    "        market_level_columns + [\"day_market\", \"month_market\"],\n",
    "        [\"\"],\n",
    "        [\"\"]\n",
    "    ]\n",
    "    reduced_df_lv_1.columns = pd.MultiIndex.from_product(even_level_1_arrays, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "\n",
    "    # Level 2: ufi dependent columns\n",
    "    index_cols = index_cols + [\"ufi\"]\n",
    "    ufi_level_columns = [\"p_max\", \"p_min\", \"telemetry\", \"telemetry_pct_good\", \"telemetry_open\", \"telemetry_close\", \"telemetry_min\", \"telemetry_max\", \"telemetry_std\", \"telemetry_value_count\", \"telemetry_slope\", \"lat\",\"lon\"] # \"codCliente\", \"up\", \n",
    "    reduced_df_lv_2 = multiple_line_df[ufi_level_columns + index_cols]\n",
    "    reduced_df_lv_2 = reduced_df_lv_2.drop_duplicates(subset=[\"datetime_market\",\"ufi\"])\n",
    "    reduced_df_lv_2 = reduced_df_lv_2.pivot(index=['datetime_market'], columns=['ufi'], values=ufi_level_columns)\n",
    "    # Add missing level to even the shapes\n",
    "    even_level_2_arrays = [\n",
    "        list(reduced_df_lv_2.columns.get_level_values(0)),\n",
    "        list(reduced_df_lv_2.columns.get_level_values(1)),\n",
    "        list([\"\"] * reduced_df_lv_2.columns.shape[0])\n",
    "    ]\n",
    "    even_level_2_tuples = list(zip(*even_level_2_arrays))\n",
    "    reduced_df_lv_2.columns = pd.MultiIndex.from_tuples(even_level_2_tuples, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "    reduced_df_lv_2\n",
    "\n",
    "    # Level 3: horizon dependent columns\n",
    "    index_cols = index_cols + [\"hours_fwd\"]\n",
    "    horizon_level_columns = [\"forecast\",\"metering\"] #,\"forecast_error_metering\" ,\"forecast_error_telemetry\"]\n",
    "    reduced_df_lv_3 = multiple_line_df[horizon_level_columns + index_cols]\n",
    "    reduced_df_lv_3 = reduced_df_lv_3.drop_duplicates()\n",
    "    reduced_df_lv_3 = reduced_df_lv_3.pivot(index=['datetime_market'], columns=['ufi','hours_fwd'], values=horizon_level_columns)\n",
    "\n",
    "    pivotted_df = pd.concat([reduced_df_lv_1,  pd.concat([reduced_df_lv_2, reduced_df_lv_3], axis=1)], axis=1)\n",
    "\n",
    "    return pivotted_df\n",
    "\n",
    "\n",
    "\n",
    "def add_forecast_error_pivot(pivot_df, error_reference=\"telemetry\"):\n",
    "\n",
    "    ufis_in_df = pivot_df.columns.get_level_values(\"ufi\").unique()\n",
    "    # Remove empty ufi used for even levels\n",
    "    ufis_in_df = [ufi for ufi in ufis_in_df if ufi]\n",
    "    fcst_error_df = pd.DataFrame()\n",
    "    fcst_error_df_partial = pd.DataFrame()\n",
    "\n",
    "    for ufi in ufis_in_df:\n",
    "\n",
    "        if error_reference == \"telemetry\":\n",
    "            # Telemetry aligned with index hour (it comes with 1 hour lag)\n",
    "            telemetry_market_t = pivot_df[error_reference,ufi].shift(-1)\n",
    "            # Forecasted production aligned with the index hour (we take the t+1 forecast)\n",
    "            forecast_market_t = pivot_df[\"forecast\",ufi,1].shift(1)\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = forecast_market_t - telemetry_market_t\n",
    "            # Lag the forecast error 1 hour so it is available at prediction time\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = fcst_error_df_partial[f\"forecast_error_{error_reference}\"].shift(1)\n",
    "        else:\n",
    "            # Error with respect to Metering  which is already aligned\n",
    "            metering_market_t = pivot_df[error_reference,ufi,1]\n",
    "            forecast_market_t = pivot_df[\"forecast\",ufi,1]\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = forecast_market_t - metering_market_t\n",
    "            # Lag the forecast error 1 hour so it is available at prediction time\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = fcst_error_df_partial[f\"forecast_error_{error_reference}\"]\n",
    "\n",
    "\n",
    "        fcst_error_df_partial[\"ufi\"] = ufi\n",
    "        fcst_error_df = pd.concat([fcst_error_df, fcst_error_df_partial])\n",
    "\n",
    "    fcst_error_df = fcst_error_df.pivot(columns=['ufi'], values=[f\"forecast_error_{error_reference}\"])\n",
    "\n",
    "    # Add missing level to even the shapes\n",
    "    even_level_2_arrays = [\n",
    "        list(fcst_error_df.columns.get_level_values(0)),\n",
    "        list(fcst_error_df.columns.get_level_values(1)),\n",
    "        list([\"\"] * fcst_error_df.columns.shape[0])\n",
    "    ]\n",
    "    even_level_2_tuples = list(zip(*even_level_2_arrays))\n",
    "    fcst_error_df.columns = pd.MultiIndex.from_tuples(even_level_2_tuples, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "\n",
    "    return pd.concat([fcst_error_df, pivot_df], axis=1)\n",
    "\n",
    "\n",
    "def get_master(date_from, date_to, cols_to_keep, horizons, ufis, values_to_pivot, do_pivot=True):\n",
    "\n",
    "    # Load premaster data\n",
    "    config_dict = Config.get_config()\n",
    "    factory = DataAccessFactory()\n",
    "    data_config = config_dict[\"data_access_factory\"]\n",
    "    source = factory.get(data_config[\"master_overcost\"][\"source\"])\n",
    "\n",
    "    master = utils_io.load_monthly(\n",
    "        path=f\"forecast/research/premaster_eolic\",\n",
    "        date_col=\"date\",\n",
    "        date_from=date_from,\n",
    "        date_to=date_to,\n",
    "        data_access=source,\n",
    "    )\n",
    "\n",
    "    # Get sample of premaster\n",
    "    if cols_to_keep == \"all\":\n",
    "        cols_to_keep = master.columns\n",
    "    reduced_df = master[cols_to_keep]\n",
    "    # Get only info for the next three hours\n",
    "    reduced_df = reduced_df[reduced_df[\"hours_fwd\"].isin(horizons)]\n",
    "    # Get only records for target ufis\n",
    "    reduced_df = reduced_df[reduced_df[\"ufi\"].isin(ufis)][cols_to_keep]\n",
    "    # Drop columns with empty meterings\n",
    "    reduced_df = reduced_df[reduced_df['metering'].notna()]\n",
    "    # Add forecast_error_predict_time\n",
    "#     reduced_df[\"forecast_error_metering\"] = reduced_df[\"forecast\"] - reduced_df[\"metering\"]\n",
    "\n",
    "    # The telemetry is not aligned with the forecast thus we cannot simply subtract\n",
    "    #     reduced_df[\"forecast_error_telemetry\"] = reduced_df[\"forecast\"] - reduced_df[\"telemetry\"]\n",
    "\n",
    "    # ?Drop rows with empty forecast error since we cannot know their real values \n",
    "    reduced_df = reduced_df.drop_duplicates()\n",
    "    if do_pivot:\n",
    "        pivot_df = pivot_master_by_levels(reduced_df)\n",
    "    else:\n",
    "        return reduced_df\n",
    "\n",
    "    # Now we can align the forecasts and telemetry at market time to get the recent forecast error\n",
    "    pivot_df = add_forecast_error_pivot(pivot_df, error_reference=\"telemetry\")\n",
    "    pivot_df = add_forecast_error_pivot(pivot_df, error_reference=\"metering\")\n",
    "\n",
    "    # It's really important to determine the order of the columns since we will be working with their array representation, not the dataframe\n",
    "    pivot_df = pivot_df.sort_index(axis='columns', level=[0,1,2])\n",
    "\n",
    "    return pivot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97e2a8",
   "metadata": {},
   "source": [
    "### Get ufi coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d04130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ufis_location(df_with_locations):\n",
    "    '''\n",
    "    :param df_with_locations: master table with lat lon columns\n",
    "    \n",
    "    Receives the tabular master with temporal index\n",
    "    It sumarizes the coord info of each ufi in a dataframe identified by an id\n",
    "    In case some ufis have the same coords a slight offset is applied to them\n",
    "    so they don't share the exact location\n",
    "    \n",
    "    :return ufi_coords_df: summary of each ufi's location identified by name and\n",
    "    a numeric id\n",
    "    '''\n",
    "    \n",
    "    # Get ufi list\n",
    "    ufi_list = df_with_locations.columns.get_level_values(1)\n",
    "    # Filter out empty ufi used for  index levels and remove duplicates\n",
    "    ufi_list = list(filter(None, ufi_list.unique()))\n",
    "\n",
    "    ufi_coord_df = pd.DataFrame()\n",
    "    for ufi in ufi_list:\n",
    "        # Use the tail since the head has some nans\n",
    "        lat_ufi = df_with_locations[~np.isnan(df_with_locations[\"lat\"][ufi])][\"lat\"][ufi].values[0]\n",
    "        lon_ufi = df_with_locations[~np.isnan(df_with_locations[\"lon\"][ufi])][\"lon\"][ufi].values[0]\n",
    "\n",
    "        ufi_info = pd.Series([ufi,lat_ufi,lon_ufi])\n",
    "        ufi_coord_df = ufi_coord_df.append(ufi_info, ignore_index=True)\n",
    "\n",
    "    ufi_coord_df.columns = [\"ufi\", \"lat\", \"lon\"]\n",
    "\n",
    "    # Offset ufis sharing location\n",
    "    repeated_coords = ufi_coord_df.groupby([\"lat\", \"lon\"]).agg(set).reset_index()\n",
    "    for lat,lon,ufis in repeated_coords[repeated_coords[\"ufi\"].apply(lambda x: len(x)) > 1].values:\n",
    "\n",
    "        offset = 1e-5\n",
    "        for ufi in list(ufis):\n",
    "            print(f\"Offsetting {ufi}...\")\n",
    "            ufi_coord_df.loc[ufi_coord_df[\"ufi\"] == ufi,\"lat\"] = lat + offset\n",
    "            offset = offset + 1e-5\n",
    "    ufi_coord_df = ufi_coord_df.sort_values(\"ufi\").reset_index(drop=True).reset_index() \n",
    "    ufi_coord_df = ufi_coord_df.rename(columns={\"index\": \"id\"})\n",
    "    \n",
    "    \n",
    "    return ufi_coord_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573a432",
   "metadata": {},
   "source": [
    "### Get grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a28bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_ufis_to_grid(df_coord_summary):\n",
    "    '''\n",
    "    : param df_coord_summary: info with each ufi of the data set\n",
    "                              with its coordinates and an id\n",
    "                              \n",
    "    Maps the coordinates of the ufis to a narrower space to reduce\n",
    "    the sparsity of the initial map.\n",
    "\n",
    "    :return grid: 2D matrix representing the location of each\n",
    "                  ufi. The values of each index is the id of the \n",
    "                  ufi located in it. In case no there is no ufi\n",
    "                  the id is -1.\n",
    "    '''\n",
    "    sorted_latitudes = df_coord_summary[\"lat\"].unique()\n",
    "    sorted_latitudes.sort()\n",
    "\n",
    "    sorted_longitudes = df_coord_summary[\"lon\"].unique()\n",
    "    sorted_longitudes.sort()\n",
    "\n",
    "    grid_shape = [len(sorted_latitudes), len(sorted_longitudes)]\n",
    "    grid = np.ones(grid_shape)\n",
    "    grid = grid * -1\n",
    "\n",
    "\n",
    "    for id, ufi, lat, lon in df_coord_summary.values:\n",
    "        id_x = np.where(sorted_latitudes == lat)[0][0]\n",
    "        id_y = np.where(sorted_longitudes == lon)[0][0]\n",
    "        grid[id_x][id_y] = id\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366dbb7",
   "metadata": {},
   "source": [
    "## Generate grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = get_master(\"2019-01-01\", \"2022-11-01\", cols_to_keep=\"all\", horizons=target_hours_fwd, ufis=target_ufis, values_to_pivot=info_columns)\n",
    "ufi_location_summary = get_ufis_location(master)\n",
    "ufi_grid = embed_ufis_to_grid(ufi_location_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d92d46c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master.columns.get_level_values(1).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8138022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 47)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufi_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea5adf",
   "metadata": {},
   "source": [
    "## Save grid and location summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cf1b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufi_location_summary.to_csv(\"data/tfm/scenes/ufi_location_summary.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ddb205",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tfm/scenes/ufi_grid.npy', 'wb') as f:\n",
    "    np.save(f, ufi_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fc9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc58ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d0821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4035d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:windTFM]",
   "language": "python",
   "name": "conda-env-windTFM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
