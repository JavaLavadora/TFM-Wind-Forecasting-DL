{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64681fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from common_utils.utils.config import Config\n",
    "from common_utils.io.data_access.data_access_factory import DataAccessFactory\n",
    "\n",
    "# from axpo_trading.forecast.forecast_preprocess_iberia import preproces_ufis\n",
    "from common_utils.utils import utils, utils_io, utils_date\n",
    "from axpo_trading.forecast import forecast_sql_preprocess_iberia\n",
    "from axpo_trading.forecast import forecast_preprocess_iberia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random seeds\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(42)\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "set_random_seed(42)\n",
    "import random as rn\n",
    "\n",
    "rn.seed(1254)\n",
    "from keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "# wind_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "wind_path = \"/home/jovyan/projects/AdvancedAnalytics-UseCase-Wind\"\n",
    "os.chdir(wind_path)\n",
    "\n",
    "os.environ[\"CONFIG_DIR\"] = \"config_files\"\n",
    "os.environ[\"AUTH_CONFIG_DIR\"] = \"auth\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_CONTAINER_NAME_WIND_RAW\"] = \"raw\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_CONTAINER_NAME_WIND_STAGING\"] = \"staging\"\n",
    "os.environ[\n",
    "    \"AZURE_SQL_SHARED_RAW_SERVER\"\n",
    "] = \"axso-prod-appl-aa-prod-shared-sql-secondary.database.windows.net\"\n",
    "# os.environ[\"AZURE_SQL_SHARED_RAW_SERVER\"] = 'axso-prod-appl-aa-prod-shared-sql.database.windows.net'\n",
    "os.environ[\"AZURE_SQL_SHARED_RAW_DATABASE\"] = \"axso-prod-appl-aa-prod-shared-raw-sqldb\"\n",
    "os.environ[\"N_THREADS_SQL\"] = \"1\"\n",
    "\n",
    "# DEV\n",
    "os.environ[\"ENV\"] = \"azure_iberia_k8s_dev\"\n",
    "# BLOB DEV\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_NAME\"] = \"axsonpaadevdslabdls\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_RAW_NAME\"] = \"axsoprodaaprodshareddls-secondary\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_CONTAINER_NAME_WIND_REFINED\"] = \"wind-refined\"\n",
    "os.environ[\"AZURE_STORAGE_ACCOUNT_DATA_CONTAINER_NAME_WIND_RESULTS\"] = \"wind-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afe51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 12\n",
    "n_steps_out = 3\n",
    "\n",
    "portfolio_level = True\n",
    "\n",
    "main_premaster_columns = [\"datetime_market\",\"datetime\",\"hours_fwd\",\"ufi\",\"telemetry\",\"forecast\",\"metering\"]\n",
    "info_columns = [\"telemetry\",\"forecast\",\"metering\",\"forecast_error_metering\",\"forecast_error_telemetry\"]\n",
    "groupping_columns = ['ufi','hours_fwd']\n",
    "target_hours_fwd = [1,2,3]\n",
    "# target_ufis = [\"ZAPATER\",\"PEARBO\",\"ROMERA\"]\n",
    "# target_ufis = [\"ABELLA\",\"PAXAMON\",\"SPADRON\",\"PELALIN\",\"TIGUEIR\",\"PEIRIXO\",\"MONTCEO\",\"MONTOUT\"]\n",
    "target_ufis= ['ABELLA', 'CERROS', 'LAMESA', 'LACAYA', 'VILACHA', 'TIGUEIR',\n",
    "       'ESQUILE', 'BRULLES', 'PELALIN', 'PESLOB', 'VISOS', 'DEFERII',\n",
    "       'PECORTI', 'LASORDA', 'ESCANDO', 'BAYO', 'HINOJII',\n",
    "       'PEOCHAO', 'CALERA', 'CPELAOS', 'ELGALLO', 'SPADRON', 'PAXAMON',\n",
    "       'TRAPERA', 'SABUCED', 'PEZARZU', 'PESLOA', 'ASNEVES', 'CAMPANA',\n",
    "       'PECOUTE', 'HINOJAI', 'PESLOD', 'AXIABRE', 'FEIXOS', 'OTERO',\n",
    "       'POTRA', 'ZARZUEL', 'CERCEDA', 'GRAIADE', 'PEOUROL', 'RODERA',\n",
    "       'MONTOUT', 'ARTEIXO', 'ELLLAN', 'MONTCEO', 'LALOMBA', 'CARRACE',\n",
    "       'PEIRIXO', 'ATALAYA', 'FRAILA', 'DEHESII', 'MONTERO', 'MONDONE',\n",
    "       'ROMERA', 'ESE', 'BANDELE', 'SANJOSE', 'SERRETA', 'DEHEII',\n",
    "       'AEROGEN', 'ZAPATER', 'LARUYA', 'PESLOC', 'PEARBO', 'PELALOM',\n",
    "       'MUDEFER']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7532e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc9f52",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Load master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4fd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_master_by_levels(multiple_line_df):\n",
    "\n",
    "    # TODO Parametrize levels\n",
    "\n",
    "    # Pivot data according to level\n",
    "    # Level 1: market dependent columns\n",
    "    index_cols = [\"datetime_market\"]\n",
    "    market_level_columns = [\"hour_market\"]\n",
    "    reduced_df_lv_1 = multiple_line_df[market_level_columns + index_cols]\n",
    "    reduced_df_lv_1[\"day_market\"] = multiple_line_df[\"datetime_market\"].dt.day\n",
    "    reduced_df_lv_1[\"month_market\"] = multiple_line_df[\"datetime_market\"].dt.month\n",
    "    # Add missing levels to even the final shapes\n",
    "    reduced_df_lv_1 = reduced_df_lv_1.drop_duplicates()\n",
    "    reduced_df_lv_1 = reduced_df_lv_1.set_index(index_cols, drop=True)\n",
    "    even_level_1_arrays = [\n",
    "        market_level_columns + [\"day_market\", \"month_market\"],\n",
    "        [\"\"],\n",
    "        [\"\"]\n",
    "    ]\n",
    "    reduced_df_lv_1.columns = pd.MultiIndex.from_product(even_level_1_arrays, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "\n",
    "    # Level 2: ufi dependent columns\n",
    "    index_cols = index_cols + [\"ufi\"]\n",
    "    ufi_level_columns = [\"p_max\", \"p_min\", \"telemetry\", \"telemetry_pct_good\", \"telemetry_open\", \"telemetry_close\", \"telemetry_min\", \"telemetry_max\", \"telemetry_std\", \"telemetry_value_count\", \"telemetry_slope\", \"lat\",\"lon\"] # \"codCliente\", \"up\", \n",
    "    reduced_df_lv_2 = multiple_line_df[ufi_level_columns + index_cols]\n",
    "    reduced_df_lv_2 = reduced_df_lv_2.drop_duplicates(subset=[\"datetime_market\",\"ufi\"])\n",
    "    reduced_df_lv_2 = reduced_df_lv_2.pivot(index=['datetime_market'], columns=['ufi'], values=ufi_level_columns)\n",
    "    # Add missing level to even the shapes\n",
    "    even_level_2_arrays = [\n",
    "        list(reduced_df_lv_2.columns.get_level_values(0)),\n",
    "        list(reduced_df_lv_2.columns.get_level_values(1)),\n",
    "        list([\"\"] * reduced_df_lv_2.columns.shape[0])\n",
    "    ]\n",
    "    even_level_2_tuples = list(zip(*even_level_2_arrays))\n",
    "    reduced_df_lv_2.columns = pd.MultiIndex.from_tuples(even_level_2_tuples, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "    reduced_df_lv_2\n",
    "\n",
    "    # Level 3: horizon dependent columns\n",
    "    index_cols = index_cols + [\"hours_fwd\"]\n",
    "    horizon_level_columns = [\"forecast\",\"metering\"] #,\"forecast_error_metering\" ,\"forecast_error_telemetry\"]\n",
    "    reduced_df_lv_3 = multiple_line_df[horizon_level_columns + index_cols]\n",
    "    reduced_df_lv_3 = reduced_df_lv_3.drop_duplicates()\n",
    "    reduced_df_lv_3 = reduced_df_lv_3.pivot(index=['datetime_market'], columns=['ufi','hours_fwd'], values=horizon_level_columns)\n",
    "\n",
    "    pivotted_df = pd.concat([reduced_df_lv_1,  pd.concat([reduced_df_lv_2, reduced_df_lv_3], axis=1)], axis=1)\n",
    "\n",
    "    return pivotted_df\n",
    "\n",
    "\n",
    "\n",
    "def add_forecast_error_pivot(pivot_df, error_reference=\"telemetry\"):\n",
    "\n",
    "    ufis_in_df = pivot_df.columns.get_level_values(\"ufi\").unique()\n",
    "    # Remove empty ufi used for even levels\n",
    "    ufis_in_df = [ufi for ufi in ufis_in_df if ufi]\n",
    "    fcst_error_df = pd.DataFrame()\n",
    "    fcst_error_df_partial = pd.DataFrame()\n",
    "\n",
    "    for ufi in ufis_in_df:\n",
    "\n",
    "        if error_reference == \"telemetry\":\n",
    "            # Telemetry aligned with index hour (it comes with 1 hour lag)\n",
    "            telemetry_market_t = pivot_df[error_reference,ufi].shift(-1)\n",
    "            # Forecasted production aligned with the index hour (we take the t+1 forecast)\n",
    "            forecast_market_t = pivot_df[\"forecast\",ufi,1].shift(1)\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = forecast_market_t - telemetry_market_t\n",
    "            # Lag the forecast error 1 hour so it is available at prediction time\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = fcst_error_df_partial[f\"forecast_error_{error_reference}\"].shift(1)\n",
    "        else:\n",
    "            # Error with respect to Metering  which is already aligned\n",
    "            metering_market_t = pivot_df[error_reference,ufi,1]\n",
    "            forecast_market_t = pivot_df[\"forecast\",ufi,1]\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = forecast_market_t - metering_market_t\n",
    "            # Lag the forecast error 1 hour so it is available at prediction time\n",
    "            fcst_error_df_partial[f\"forecast_error_{error_reference}\"] = fcst_error_df_partial[f\"forecast_error_{error_reference}\"]\n",
    "\n",
    "\n",
    "        fcst_error_df_partial[\"ufi\"] = ufi\n",
    "        fcst_error_df = pd.concat([fcst_error_df, fcst_error_df_partial])\n",
    "\n",
    "    fcst_error_df = fcst_error_df.pivot(columns=['ufi'], values=[f\"forecast_error_{error_reference}\"])\n",
    "\n",
    "    # Add missing level to even the shapes\n",
    "    even_level_2_arrays = [\n",
    "        list(fcst_error_df.columns.get_level_values(0)),\n",
    "        list(fcst_error_df.columns.get_level_values(1)),\n",
    "        list([\"\"] * fcst_error_df.columns.shape[0])\n",
    "    ]\n",
    "    even_level_2_tuples = list(zip(*even_level_2_arrays))\n",
    "    fcst_error_df.columns = pd.MultiIndex.from_tuples(even_level_2_tuples, names=[\"feature\", \"ufi\", \"hours_fwd\"])\n",
    "\n",
    "    return pd.concat([fcst_error_df, pivot_df], axis=1)\n",
    "\n",
    "\n",
    "def get_master(date_from, date_to, cols_to_keep, horizons, ufis, values_to_pivot, do_pivot=True):\n",
    "\n",
    "    # Load premaster data\n",
    "    config_dict = Config.get_config()\n",
    "    factory = DataAccessFactory()\n",
    "    data_config = config_dict[\"data_access_factory\"]\n",
    "    source = factory.get(data_config[\"master_overcost\"][\"source\"])\n",
    "\n",
    "    master = utils_io.load_monthly(\n",
    "        path=f\"forecast/research/premaster_eolic\",\n",
    "        date_col=\"date\",\n",
    "        date_from=date_from,\n",
    "        date_to=date_to,\n",
    "        data_access=source,\n",
    "    )\n",
    "\n",
    "    # Get sample of premaster\n",
    "    if cols_to_keep == \"all\":\n",
    "        cols_to_keep = master.columns\n",
    "    reduced_df = master[cols_to_keep]\n",
    "    # Get only info for the next three hours\n",
    "    reduced_df = reduced_df[reduced_df[\"hours_fwd\"].isin(horizons)]\n",
    "    # Get only records for target ufis\n",
    "    reduced_df = reduced_df[reduced_df[\"ufi\"].isin(ufis)][cols_to_keep]\n",
    "    # Drop columns with empty meterings\n",
    "    reduced_df = reduced_df[reduced_df['metering'].notna()]\n",
    "    # Add forecast_error_predict_time\n",
    "#     reduced_df[\"forecast_error_metering\"] = reduced_df[\"forecast\"] - reduced_df[\"metering\"]\n",
    "\n",
    "    # The telemetry is not aligned with the forecast thus we cannot simply subtract\n",
    "    #     reduced_df[\"forecast_error_telemetry\"] = reduced_df[\"forecast\"] - reduced_df[\"telemetry\"]\n",
    "\n",
    "    # ?Drop rows with empty forecast error since we cannot know their real values \n",
    "    reduced_df = reduced_df.drop_duplicates()\n",
    "    if do_pivot:\n",
    "        pivot_df = pivot_master_by_levels(reduced_df)\n",
    "    else:\n",
    "        return reduced_df\n",
    "\n",
    "    # Now we can align the forecasts and telemetry at market time to get the recent forecast error\n",
    "    pivot_df = add_forecast_error_pivot(pivot_df, error_reference=\"telemetry\")\n",
    "    pivot_df = add_forecast_error_pivot(pivot_df, error_reference=\"metering\")\n",
    "\n",
    "    # It's really important to determine the order of the columns since we will be working with their array representation, not the dataframe\n",
    "    pivot_df = pivot_df.sort_index(axis='columns', level=[0,1,2])\n",
    "\n",
    "    return pivot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3e8e7",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Split Sequences (time rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e9e1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps_in, n_steps_out, target_col, portfolio_level=True):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences.iloc[i:end_ix, :], sequences.iloc[end_ix:out_end_ix, :]\n",
    "\n",
    "        # As a feature we have forecast error based on telemetry (real time proxy for meterings)\n",
    "        seq_x = seq_x.rename(columns={\"forecast_error_telemetry\":\"forecast_error\"})\n",
    "        # As a target we want the forecast error to be the based on the actual metering\n",
    "        seq_y = seq_y.rename(columns={\"forecast_error_metering\":\"forecast_error\"})\n",
    "\n",
    "        # Drop the spare forecast error and original meterings\n",
    "        seq_x = seq_x.drop([\"forecast_error_metering\",\"metering\"], axis=1)\n",
    "\n",
    "        # OPTION 1. Predict all input time series\n",
    "        # seq_y = seq_y.drop([\"forecast_error_telemetry\",\"metering\"], axis=1)\n",
    "        # OPTION 2. Predict only target col for each UFI\n",
    "        seq_y = seq_y[target_col]\n",
    "        # OPTION 3. Predict only taget col for complete Portfolio\n",
    "        if portfolio_level:\n",
    "            if target_col == \"forecast\":\n",
    "                seq_y = seq_y.loc[:, (slice(None),1)]\n",
    "            seq_y = seq_y.sum(axis=1)\n",
    "\n",
    "        # Flatten col levels\n",
    "        flatten_cols_x = [\"-\".join(str(cs) for cs in c) for c in seq_x.columns.to_series()]\n",
    "        seq_x.columns = flatten_cols_x\n",
    "        if not portfolio_level:\n",
    "            flatten_cols_y = [\"-\".join(str(cs) for cs in c) for c in seq_y.columns.to_series()]\n",
    "            seq_y.columns = flatten_cols_y\n",
    "\n",
    "        X.append(seq_x.values)\n",
    "        y.append(seq_y.values)\n",
    "\n",
    "    X_arr = array(X)\n",
    "    y_arr = array(y)\n",
    "\n",
    "    if portfolio_level:\n",
    "        y_arr = y_arr[:,:, np.newaxis]\n",
    "\n",
    "    return X_arr, y_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d8fba",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Scene Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e80c2",
   "metadata": {},
   "source": [
    "First we need a dataframe with the locations of each ufi.\n",
    "Since we have some ufi sharing location (probably because the coords are specified at up level) we need to slightly offset them so that the algorithm can differentiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60fb3ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 11:05:36,472 - MainThread - [INFO] - b'[UTILS IO] - load_monthly at line 176: Loading monthly from : forecast/research/premaster_eolic'\n",
      "2023-01-27 11:05:36,507 - MainThread - [INFO] - b'[UTILS IO] - format_dates at line 465: 2018-12-31'\n",
      "2023-01-27 11:05:36,652 - MainThread - [INFO] - b\"[UTILS IO] - filter_files_load_monthly at line 448: Files in path: ['premaster_eolic___202209_0.h5', 'premaster_eolic___202210_0.h5', 'premaster_eolic___202009_0.h5', 'premaster_eolic___202101_0.h5', 'premaster_eolic___202008_0.h5', 'premaster_eolic___201912_0.h5', 'premaster_eolic___202104_0.h5', 'premaster_eolic___201905_0.h5', 'premaster_eolic___202102_0.h5', 'premaster_eolic___202205_0.h5', 'premaster_eolic___202207_0.h5', 'premaster_eolic___202012_0.h5', 'premaster_eolic___202005_0.h5', 'premaster_eolic___202002_0.h5', 'premaster_eolic___202006_0.h5', 'premaster_eolic___201904_0.h5', 'premaster_eolic___202208_0.h5', 'premaster_eolic___201906_0.h5', 'premaster_eolic___202001_0.h5', 'premaster_eolic___201903_0.h5', 'premaster_eolic___201907_0.h5', 'premaster_eolic___202007_0.h5', 'premaster_eolic___201908_0.h5', 'premaster_eolic___201909_0.h5', 'premaster_eolic___202201_0.h5', 'premaster_eolic___202206_0.h5', 'premaster_eolic___201901_0.h5', 'premaster_eolic___202106_0.h5', 'premaster_eolic___202110_0.h5', 'premaster_eolic___202111_0.h5', 'premaster_eolic___202107_0.h5', 'premaster_eolic___202011_0.h5', 'premaster_eolic___202112_0.h5', 'premaster_eolic___202003_0.h5', 'premaster_eolic___202202_0.h5', 'premaster_eolic___201910_0.h5', 'premaster_eolic___202109_0.h5', 'premaster_eolic___202103_0.h5', 'premaster_eolic___201911_0.h5', 'premaster_eolic___202004_0.h5', 'premaster_eolic___202204_0.h5', 'premaster_eolic___202108_0.h5', 'premaster_eolic___202105_0.h5', 'premaster_eolic___201902_0.h5', 'premaster_eolic___202203_0.h5', 'premaster_eolic___202010_0.h5']\"\n",
      "2023-01-27 11:05:36,693 - MainThread - [INFO] - b\"[UTILS IO] - filter_files_load_monthly at line 451: dates: [Timestamp('2022-09-01 00:00:00'), Timestamp('2022-10-01 00:00:00'), Timestamp('2020-09-01 00:00:00'), Timestamp('2021-01-01 00:00:00'), Timestamp('2020-08-01 00:00:00'), Timestamp('2019-12-01 00:00:00'), Timestamp('2021-04-01 00:00:00'), Timestamp('2019-05-01 00:00:00'), Timestamp('2021-02-01 00:00:00'), Timestamp('2022-05-01 00:00:00'), Timestamp('2022-07-01 00:00:00'), Timestamp('2020-12-01 00:00:00'), Timestamp('2020-05-01 00:00:00'), Timestamp('2020-02-01 00:00:00'), Timestamp('2020-06-01 00:00:00'), Timestamp('2019-04-01 00:00:00'), Timestamp('2022-08-01 00:00:00'), Timestamp('2019-06-01 00:00:00'), Timestamp('2020-01-01 00:00:00'), Timestamp('2019-03-01 00:00:00'), Timestamp('2019-07-01 00:00:00'), Timestamp('2020-07-01 00:00:00'), Timestamp('2019-08-01 00:00:00'), Timestamp('2019-09-01 00:00:00'), Timestamp('2022-01-01 00:00:00'), Timestamp('2022-06-01 00:00:00'), Timestamp('2019-01-01 00:00:00'), Timestamp('2021-06-01 00:00:00'), Timestamp('2021-10-01 00:00:00'), Timestamp('2021-11-01 00:00:00'), Timestamp('2021-07-01 00:00:00'), Timestamp('2020-11-01 00:00:00'), Timestamp('2021-12-01 00:00:00'), Timestamp('2020-03-01 00:00:00'), Timestamp('2022-02-01 00:00:00'), Timestamp('2019-10-01 00:00:00'), Timestamp('2021-09-01 00:00:00'), Timestamp('2021-03-01 00:00:00'), Timestamp('2019-11-01 00:00:00'), Timestamp('2020-04-01 00:00:00'), Timestamp('2022-04-01 00:00:00'), Timestamp('2021-08-01 00:00:00'), Timestamp('2021-05-01 00:00:00'), Timestamp('2019-02-01 00:00:00'), Timestamp('2022-03-01 00:00:00'), Timestamp('2020-10-01 00:00:00')]\"\n",
      "2023-01-27 11:05:36,728 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201901_0.h5'\n",
      "2023-01-27 11:05:41,667 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201902_0.h5'\n",
      "2023-01-27 11:05:45,849 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201903_0.h5'\n",
      "2023-01-27 11:05:50,788 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201904_0.h5'\n",
      "2023-01-27 11:05:55,853 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201905_0.h5'\n",
      "2023-01-27 11:06:00,847 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201906_0.h5'\n",
      "2023-01-27 11:06:05,813 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201907_0.h5'\n",
      "2023-01-27 11:06:10,988 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201908_0.h5'\n",
      "2023-01-27 11:06:16,115 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201909_0.h5'\n",
      "2023-01-27 11:06:21,785 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201910_0.h5'\n",
      "2023-01-27 11:06:27,436 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201911_0.h5'\n",
      "2023-01-27 11:06:32,752 - MainThread - [INFO] - b'[UTILS IO] - get_files_without_sampling at line 398: Reading file premaster_eolic___201912_0.h5'\n"
     ]
    }
   ],
   "source": [
    "# date_first = \"2018-12-31\"\n",
    "# date_last = \"2022-10-31\"\n",
    "date_first = \"2018-12-31\"\n",
    "date_last = \"2019-12-31\"\n",
    "master = get_master(date_first, date_last, cols_to_keep=\"all\", horizons=target_hours_fwd, ufis=target_ufis, values_to_pivot=info_columns)\n",
    "master = master.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebcea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_bl = master.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8cb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = master_bl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc765b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offsetting HINOJII...\n",
      "Offsetting HINOJAI...\n",
      "Offsetting ZARZUEL...\n",
      "Offsetting DEFERII...\n",
      "Offsetting MUDEFER...\n",
      "Offsetting FEIXOS...\n",
      "Offsetting POTRA...\n",
      "Offsetting BANDELE...\n",
      "Offsetting RODERA...\n",
      "Offsetting PESLOB...\n",
      "Offsetting PESLOD...\n",
      "Offsetting PESLOC...\n",
      "Offsetting PESLOA...\n",
      "Offsetting BAYO...\n",
      "Offsetting ELLLAN...\n",
      "Offsetting DEHESII...\n",
      "Offsetting DEHEII...\n",
      "Offsetting MONTOUT...\n",
      "Offsetting PEARBO...\n",
      "Offsetting ZAPATER...\n",
      "Offsetting ELGALLO...\n",
      "Offsetting PEZARZU...\n",
      "Offsetting FRAILA...\n",
      "Offsetting LACAYA...\n",
      "Offsetting LAMESA...\n",
      "Offsetting OTERO...\n",
      "Offsetting PEOUROL...\n",
      "Offsetting PECOUTE...\n",
      "Offsetting SABUCED...\n",
      "Offsetting VILACHA...\n"
     ]
    }
   ],
   "source": [
    "# Get ufi list\n",
    "ufi_list = master.columns.get_level_values(1)\n",
    "# Filter out empty ufi used for  index levels and remove duplicates\n",
    "ufi_list = list(filter(None, ufi_list.unique()))\n",
    "\n",
    "ufi_coord_df = pd.DataFrame()\n",
    "for ufi in ufi_list:\n",
    "    # Use the tail since the head has some nans\n",
    "    lat_ufi = master[~np.isnan(master[\"lat\"][ufi])][\"lat\"][ufi].values[0]\n",
    "    lon_ufi = master[~np.isnan(master[\"lon\"][ufi])][\"lon\"][ufi].values[0]\n",
    "\n",
    "    ufi_info = pd.Series([ufi,lat_ufi,lon_ufi])\n",
    "    ufi_coord_df = ufi_coord_df.append(ufi_info, ignore_index=True)\n",
    "\n",
    "ufi_coord_df.columns = [\"ufi\", \"lat\", \"lon\"]\n",
    "\n",
    "repeated_coords = ufi_coord_df.groupby([\"lat\", \"lon\"]).agg(set).reset_index()\n",
    "for lat,lon,ufis in repeated_coords[repeated_coords[\"ufi\"].apply(lambda x: len(x)) > 1].values:\n",
    "\n",
    "    offset = 1e-5\n",
    "    for ufi in list(ufis):\n",
    "        print(f\"Offsetting {ufi}...\")\n",
    "        ufi_coord_df.loc[ufi_coord_df[\"ufi\"] == ufi,\"lat\"] = lat + offset\n",
    "        offset = offset + 1e-5\n",
    "ufi_coord_df = ufi_coord_df.sort_values(\"ufi\").reset_index(drop=True).reset_index() \n",
    "ufi_coord_df = ufi_coord_df.rename(columns={\"index\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a491e7",
   "metadata": {},
   "source": [
    "Check that there are no more duplicated coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8788087f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ufi</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, ufi, lat, lon]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufi_coord_df.groupby([\"lat\",\"lon\"]).filter(lambda x: x.count()[\"ufi\"]>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6602d",
   "metadata": {},
   "source": [
    "### Grid embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5397c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_latitudes = ufi_coord_df[\"lat\"].unique()\n",
    "sorted_latitudes.sort()\n",
    "\n",
    "sorted_longitudes = ufi_coord_df[\"lon\"].unique()\n",
    "sorted_longitudes.sort()\n",
    "\n",
    "grid_shape = [len(sorted_latitudes), len(sorted_longitudes)]\n",
    "grid = np.ones(grid_shape)\n",
    "grid = grid * -1\n",
    "\n",
    "\n",
    "for id, ufi, lat, lon in ufi_coord_df.values:\n",
    "    id_x = np.where(sorted_latitudes == lat)[0][0]\n",
    "    id_y = np.where(sorted_longitudes == lon)[0][0]\n",
    "    grid[id_x][id_y] = id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46920f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.,\n",
       "       12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.,\n",
       "       25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37.,\n",
       "       38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50.,\n",
       "       51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff30d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf491132",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Picture Master Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "591a01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flatten col levels\n",
    "# master_og_cols = master.columns\n",
    "# flatten_cols_x = [\"-\".join(str(cs) for cs in c) for c in master.columns.to_series()]\n",
    "# master.columns = flatten_cols_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd51d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_scenes_master(master_pivot, grid_coords, localized_ufis):\n",
    "    \"\"\"\n",
    "    :param master_pivot: data to train in pivotted format (type: pd.DataFrame)\n",
    "    :param grid_coords: grid of size unique_latitutes X unique longitudes\n",
    "                        with the id of the ufi located in each index\n",
    "                        (type: np.array)\n",
    "    :param localized_ufis: contains corrdinate information about each ufi\n",
    "                           in master_pivot: id - ufi_name - lat - lon\n",
    "                           (type: pd.DataFrame)\n",
    "\n",
    "    : returns scene_master: Size: [time steps] X [latitudes] X [longitudes] X [features]\n",
    "              features_index: data to relable the features. Size: [features]\n",
    "    \"\"\"\n",
    "\n",
    "    example_ufi = master_pivot.columns.get_level_values(1)[-1]\n",
    "    features_index = master_pivot.iloc[0, master_pivot.columns.get_level_values(1) == example_ufi].index\n",
    "    \n",
    "    num_features = len(features_index)\n",
    "    num_time_steps = master_pivot.shape[0]\n",
    "\n",
    "    # scene_master = [time steps] X [latitudes] X [longitudes] X [features]\n",
    "    # same as image = [frame] X [width] X [length] X [channels]\n",
    "    scene_master = np.ones((num_time_steps, grid_coords.shape[0], grid_coords.shape[1], num_features))\n",
    "    scene_master = scene_master * -1\n",
    "\n",
    "\n",
    "    # Iterate over the whole grid\n",
    "    for lat_idx in np.arange(0,grid_coords.shape[0]):\n",
    "        for lon_idx in np.arange(0,grid_coords.shape[1]):\n",
    "            # Check the id for the position\n",
    "            id = grid_coords[lat_idx][lon_idx]\n",
    "            if id != -1:\n",
    "                # Lookup its corresponding ufi\n",
    "                ufi = localized_ufis[localized_ufis[\"id\"] == id][\"ufi\"].values[0]\n",
    "                # Retrieve data for ufi at time t\n",
    "                features_for_ufi_all_t = master_pivot.iloc[:, master_pivot.columns.get_level_values(1) == ufi].values\n",
    "\n",
    "                scene_master[:, lat_idx, lon_idx, :] = features_for_ufi_all_t\n",
    "\n",
    "    return scene_master, features_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e3a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_picture, features_index_picture = generate_scenes_master(master, grid, ufi_coord_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "debbfc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8758, 64, 45, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_picture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdaac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_index_picture.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e329a4b",
   "metadata": {},
   "source": [
    "#### Check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56307e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufi_to_check = \"PESLOD\"\n",
    "t_to_check = 2\n",
    "###################\n",
    "id_to_check = ufi_coord_df[ufi_coord_df[\"ufi\"] ==ufi_to_check][\"id\"].values\n",
    "coords_to_check = np.where(grid == id_to_check)\n",
    "lat_to_check = coords_to_check[0][0]\n",
    "lon_to_check = coords_to_check[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c0337b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ufi</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>PESLOD</td>\n",
       "      <td>41.70902</td>\n",
       "      <td>-7.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     ufi       lat   lon\n",
       "49  49  PESLOD  41.70902 -7.98"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufi_coord_df[ufi_coord_df[\"ufi\"] ==ufi_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72e0cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22]), array([11]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(grid == id_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4515cf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.699999809265137"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_picture[t_to_check,lat_to_check,lon_to_check,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca883609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature                   ufi     hours_fwd\n",
       "forecast                  PESLOD  1.0            5.700000\n",
       "                                  2.0            4.400000\n",
       "                                  3.0            4.000000\n",
       "forecast_error_metering   PESLOD                 4.781000\n",
       "forecast_error_telemetry  PESLOD                 4.405032\n",
       "lat                       PESLOD                41.709000\n",
       "lon                       PESLOD                -7.980000\n",
       "metering                  PESLOD  1.0            0.919000\n",
       "                                  2.0            1.531000\n",
       "                                  3.0            1.728000\n",
       "p_max                     PESLOD                33.700001\n",
       "p_min                     PESLOD                      NaN\n",
       "telemetry                 PESLOD                 2.394968\n",
       "telemetry_close           PESLOD                 2.636870\n",
       "telemetry_max             PESLOD                 3.017930\n",
       "telemetry_min             PESLOD                 1.775700\n",
       "telemetry_open            PESLOD                 2.370140\n",
       "telemetry_pct_good        PESLOD               100.000000\n",
       "telemetry_slope           PESLOD                 0.266730\n",
       "telemetry_std             PESLOD                 0.232315\n",
       "telemetry_value_count     PESLOD               624.000000\n",
       "Name: 2019-01-01 03:00:00, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Above value is 0 index which corresponds to forecast_1h_fwd\n",
    "master.iloc[t_to_check, master.columns.get_level_values(1) == ufi_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2910449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(                'forecast', 'ZARZUEL', 1.0),\n",
       "            (                'forecast', 'ZARZUEL', 2.0),\n",
       "            (                'forecast', 'ZARZUEL', 3.0),\n",
       "            ( 'forecast_error_metering', 'ZARZUEL',  ''),\n",
       "            ('forecast_error_telemetry', 'ZARZUEL',  ''),\n",
       "            (                     'lat', 'ZARZUEL',  ''),\n",
       "            (                     'lon', 'ZARZUEL',  ''),\n",
       "            (                'metering', 'ZARZUEL', 1.0),\n",
       "            (                'metering', 'ZARZUEL', 2.0),\n",
       "            (                'metering', 'ZARZUEL', 3.0),\n",
       "            (                   'p_max', 'ZARZUEL',  ''),\n",
       "            (                   'p_min', 'ZARZUEL',  ''),\n",
       "            (               'telemetry', 'ZARZUEL',  ''),\n",
       "            (         'telemetry_close', 'ZARZUEL',  ''),\n",
       "            (           'telemetry_max', 'ZARZUEL',  ''),\n",
       "            (           'telemetry_min', 'ZARZUEL',  ''),\n",
       "            (          'telemetry_open', 'ZARZUEL',  ''),\n",
       "            (      'telemetry_pct_good', 'ZARZUEL',  ''),\n",
       "            (         'telemetry_slope', 'ZARZUEL',  ''),\n",
       "            (           'telemetry_std', 'ZARZUEL',  ''),\n",
       "            (   'telemetry_value_count', 'ZARZUEL',  '')],\n",
       "           names=['feature', 'ufi', 'hours_fwd'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_index_picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0f55b",
   "metadata": {},
   "source": [
    "### Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93824b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = []\n",
    "grid = []\n",
    "ufi_coord_df = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4abe8",
   "metadata": {},
   "source": [
    "### Generate observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3733ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences_scenes(sequences, n_steps_in, n_steps_out, target_col, features_index, portfolio_level=True):\n",
    "    X, y = list(), list()\n",
    "    # Get indexes for the features to be used in each list\n",
    "    flattened_cols = [\"-\".join(str(c[i]) for i in [0,2]) for c in features_index.to_series()]\n",
    "    features_idx = [flattened_cols.index(col) for col in flattened_cols if (\"metering\" not in col) & (\"lat\" not in col) & (\"lon\" not in col)]\n",
    "    features_idy = flattened_cols.index(target_col)\n",
    "\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "\n",
    "        # Divide the master rows for the current observation: observed vs predicted\n",
    "        # Keep the features in seq_x and the target in seq_y\n",
    "        seq_x = master_picture[i:end_ix, :,:,features_idx].copy()\n",
    "        seq_y = master_picture[end_ix:out_end_ix, :,:,features_idy].copy()\n",
    "\n",
    "        # (NOT TESTED FOR Spatial model) OPTION 1. Predict all input time series\n",
    "        # seq_y = seq_y.drop([\"forecast_error_telemetry\",\"metering\"], axis=1)\n",
    "\n",
    "        # OPTION 3. Predict only taget col for complete Portfolio\n",
    "        if portfolio_level:\n",
    "            # Mask nans as -1\n",
    "            seq_y = np.nan_to_num(seq_y, nan=-1)\n",
    "            # Substitue values of non existing ufis (marked as -1) by a 0\n",
    "            np.place(seq_y, seq_y==-1, 0)\n",
    "            # Sum all target values to get the portfolio value for each timestep\n",
    "            seq_y = seq_y.sum(axis=1).sum(axis=1)\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    X_arr = array(X)\n",
    "    y_arr = array(y)\n",
    "\n",
    "    if portfolio_level:\n",
    "        y_arr = y_arr[:,:, np.newaxis]\n",
    "\n",
    "    return X_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b1a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_sequences_scenes(master_picture, n_steps_in, n_steps_in, target_col=\"forecast_error_metering-\", features_index=features_index_picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8c346a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 12, 64, 45, 15)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ee6c13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 12, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48ac22",
   "metadata": {},
   "source": [
    "#### Check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b509f018",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17230/642596540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mid_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufi_coord_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mufi_coord_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ufi\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mufi_to_check\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcoords_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mid_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlat_to_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords_to_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "ufi_to_check = \"PESLOD\"\n",
    "t_to_check = 0\n",
    "###################\n",
    "id_to_check = ufi_coord_df[ufi_coord_df[\"ufi\"] == ufi_to_check][\"id\"].values\n",
    "coords_to_check = np.where(grid == id_to_check)\n",
    "lat_to_check = coords_to_check[0][0]\n",
    "lon_to_check = coords_to_check[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f29f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufi_coord_df[ufi_coord_df[\"ufi\"] ==ufi_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d384f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(grid == id_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_picture[t_to_check,lat_to_check,lon_to_check,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above value is 0 index which corresponds to forecast_1h_fwd\n",
    "master.iloc[(t_to_check):(t_to_check + n_steps_in), master.columns.get_level_values(1) == ufi_to_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f53f75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 12, 64, 45, 15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80280c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.80000019, 6.9000001 , 5.69999981, 4.19999981, 4.        ,\n",
       "       4.5       , 4.9000001 , 4.19999981, 2.79999995, 1.10000002,\n",
       "       0.60000002, 0.1       ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[t_to_check,:,lat_to_check, lon_to_check,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fa9767a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 12, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d33c3847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.8450007 ],\n",
       "       [10.26700064],\n",
       "       [15.34799976],\n",
       "       [16.55999917],\n",
       "       [ 7.35299991],\n",
       "       [19.21099924],\n",
       "       [11.78300006],\n",
       "       [12.90299525],\n",
       "       [43.58099956],\n",
       "       [74.79400639],\n",
       "       [49.3060046 ],\n",
       "       [39.08300024]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[t_to_check,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adcbc730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-01-01 13:00:00    12.845001\n",
       "2019-01-01 14:00:00    10.267000\n",
       "2019-01-01 15:00:00    15.348000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.iloc[(n_steps_in + t_to_check):(n_steps_in + n_steps_out + t_to_check)][\"forecast_error_metering\"].fillna(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d4769",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Train Model & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3628f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import Input, Model, metrics\n",
    "from keras.layers import Dense, RepeatVector, LSTM, TimeDistributed, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e69348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the input layer with no definite frame size.\n",
    "inp = layers.Input(shape=(n_steps_in,*X_train.shape[2:]))\n",
    "\n",
    "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
    "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
    "# x = RepeatVector(n_steps_out)(inp)\n",
    "# x = layers.ConvLSTM2D(\n",
    "#     filters=21,\n",
    "#     kernel_size=(5, 5),\n",
    "#     padding=\"same\",\n",
    "#     return_sequences=True,\n",
    "#     activation=\"relu\",\n",
    "# )(inp)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(x)\n",
    "\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=10,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inp)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling3D(pool_size=(1, 3, 3), padding='same')(x)\n",
    "\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=3,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(x)\n",
    "# x = layers.Conv3D(\n",
    "#     filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "# )(x)\n",
    "# x = layers.Conv2D(1, (1,1), activation='softmax')(x)\n",
    "# x = layers.AveragePooling3D()(x)\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "x = TimeDistributed(flatten)(x)\n",
    "# outputs = tf.keras.layers.Dense(1, activation='sigmoid')(flatten1)\n",
    "dense_1 = Dense(256,activation=\"relu\")\n",
    "dense_2 = Dense(32,activation=\"relu\")\n",
    "dense_3 = Dense(1,activation=\"relu\")\n",
    "x = TimeDistributed(dense_1)(x)\n",
    "x = TimeDistributed(dense_2)(x)\n",
    "x = TimeDistributed(dense_3)(x)\n",
    "\n",
    "\n",
    "# input_layer = Input(shape=(self.n_steps_in,self.n_features_in), name='input_layer')\n",
    "# lstm_1 = LSTM(30, activation='relu', name=\"LSTM_Layer_1\")(input_layer)\n",
    "# repeat_vector = RepeatVector(self.n_steps_out, name=\"Repeating_Vector_Layer\")(lstm_1)\n",
    "# lstm_2 = LSTM(30, activation='relu', return_sequences=True, name=\"LSTM_Layer_2\")(repeat_vector)\n",
    "# lstm_3 = LSTM(10, activation='relu', return_sequences=True, name=\"LSTM_Layer_3\")(lstm_2)\n",
    "# dense = Dense(self.n_features_out, activation=\"relu\",name='Dense_Layer')\n",
    "# dropout = Dropout(.3, input_shape=(2,))(lstm_3)\n",
    "# time_dist = TimeDistributed(dense, name='Time_Distributed_Layer')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42b0fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will build the complete model and compile it.\n",
    "model = keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a05360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 12, 64, 45, 15)]  0         \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 12, 64, 45, 10)    9040      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 12, 64, 45, 10)   40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 12, 22, 15, 10)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 12, 22, 15, 3)     168       \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 12, 11, 8, 3)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 12, 264)          0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 12, 256)          67840     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 12, 32)           8224      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 12, 1)            33        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,345\n",
      "Trainable params: 85,325\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d0c0f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "62/62 [==============================] - 522s 8s/step - loss: -45.7694 - val_loss: -116.7509\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 505s 8s/step - loss: -63.3853 - val_loss: -145.5883\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 506s 8s/step - loss: -66.1240 - val_loss: -145.5883\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 504s 8s/step - loss: -66.1240 - val_loss: -145.5883\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 505s 8s/step - loss: -66.1240 - val_loss: -145.5883\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 504s 8s/step - loss: -66.1240 - val_loss: -145.5883\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 504s 8s/step - loss: -66.1240 - val_loss: -145.5883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce9e1e1810>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some callbacks to improve training.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Define modifiable training hyperparameters.\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "# Fit the model to the training data.\n",
    "size_train = np.floor(X_train.shape[0] * 0.7)\n",
    "size_validate = np.floor(X_train.shape[0] * 0.3)+1\n",
    "\n",
    "X_val = X_train[int(size_train):]\n",
    "y_val = y_train[int(size_train):]\n",
    "\n",
    "model.fit(\n",
    "    np.nan_to_num(X_train[:int(size_train)], nan=-1),\n",
    "    np.nan_to_num(y_train[:int(size_train)], nan=-1),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(np.nan_to_num(X_val, nan=-1), np.nan_to_num(y_val, nan=-1)),\n",
    "    callbacks=[early_stopping],# reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d23f47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/tfm/convlstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5fe1d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8744, 3, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1475337",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5c83c8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6120, 3, 1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:int(size_train)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d930d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 27s 329ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(np.nan_to_num(X_val, nan=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3951b5f",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e4e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaa2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ced093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "603b0988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2621, 12, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85b7e5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2621, 12, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49fbdc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8735, 12, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aad60802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8758, 1347)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bb06a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"forecast_error_metering-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f276e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_forecast = split_sequences(master.iloc[int(size_train):],n_steps_in,n_steps_in,target_col=\"forecast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "621146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_test_format = master.iloc[int(size_train):].rename(columns={\"forecast_error_metering\":\"forecast_error\"}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "00acdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_test_format[\"forecast_error_portfolio\"] = master_test_format[\"forecast_error\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "70350319",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_col_schema = [\"forecast_error_portfolio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f1d68858",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_col_schema = [\"forecast_portfolio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1be153c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = np.arange(0, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a8312fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cfb02417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6114.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ee36e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hours_fwd</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-09-12 19:00:00</th>\n",
       "      <td>13.900000</td>\n",
       "      <td>18.799999</td>\n",
       "      <td>19.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12 20:00:00</th>\n",
       "      <td>18.799999</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12 21:00:00</th>\n",
       "      <td>19.900000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>17.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12 22:00:00</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>16.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-12 23:00:00</th>\n",
       "      <td>17.799999</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>14.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 00:00:00</th>\n",
       "      <td>16.700001</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>13.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 01:00:00</th>\n",
       "      <td>14.500000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 02:00:00</th>\n",
       "      <td>13.800000</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>15.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 03:00:00</th>\n",
       "      <td>14.300000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>16.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 04:00:00</th>\n",
       "      <td>14.900000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 05:00:00</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>16.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 06:00:00</th>\n",
       "      <td>16.400000</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 07:00:00</th>\n",
       "      <td>16.200001</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>8.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 08:00:00</th>\n",
       "      <td>13.200000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 09:00:00</th>\n",
       "      <td>8.200000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>9.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 10:00:00</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 11:00:00</th>\n",
       "      <td>9.100000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>15.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 12:00:00</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>18.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 13:00:00</th>\n",
       "      <td>17.400000</td>\n",
       "      <td>20.299999</td>\n",
       "      <td>22.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-13 14:00:00</th>\n",
       "      <td>20.299999</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>21.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hours_fwd                  1.0        2.0        3.0\n",
       "2019-09-12 19:00:00  13.900000  18.799999  19.900000\n",
       "2019-09-12 20:00:00  18.799999  19.900000  19.000000\n",
       "2019-09-12 21:00:00  19.900000  19.000000  17.799999\n",
       "2019-09-12 22:00:00  19.000000  17.799999  16.700001\n",
       "2019-09-12 23:00:00  17.799999  16.700001  14.700000\n",
       "2019-09-13 00:00:00  16.700001  14.700000  13.500000\n",
       "2019-09-13 01:00:00  14.500000  13.500000  14.300000\n",
       "2019-09-13 02:00:00  13.800000  14.300000  15.600000\n",
       "2019-09-13 03:00:00  14.300000  15.600000  16.799999\n",
       "2019-09-13 04:00:00  14.900000  16.799999  16.400000\n",
       "2019-09-13 05:00:00  15.900000  16.400000  16.200001\n",
       "2019-09-13 06:00:00  16.400000  16.200001  12.600000\n",
       "2019-09-13 07:00:00  16.200001  12.600000   8.300000\n",
       "2019-09-13 08:00:00  13.200000   8.300000   7.900000\n",
       "2019-09-13 09:00:00   8.200000   7.100000   9.100000\n",
       "2019-09-13 10:00:00   7.100000   9.100000  12.500000\n",
       "2019-09-13 11:00:00   9.100000  12.500000  15.400000\n",
       "2019-09-13 12:00:00  14.200000  15.400000  18.400000\n",
       "2019-09-13 13:00:00  17.400000  20.299999  22.500000\n",
       "2019-09-13 14:00:00  20.299999  22.500000  21.400000"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_test_format[\"forecast\"][ufi_to_check].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9be0a068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.89999962, 18.79999924, 19.89999962, 19.        , 17.79999924,\n",
       "       16.70000076, 14.5       , 13.80000019, 14.30000019, 14.89999962,\n",
       "       15.89999962, 16.39999962])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[t_to_check,:,lat_to_check,lon_to_check,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f17e1549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-09-12 19:00:00    118.731003\n",
       "2019-09-12 20:00:00     72.023994\n",
       "2019-09-12 21:00:00    149.980011\n",
       "2019-09-12 22:00:00    185.714005\n",
       "2019-09-12 23:00:00    126.595993\n",
       "2019-09-13 00:00:00     86.719002\n",
       "2019-09-13 01:00:00     83.042007\n",
       "2019-09-13 02:00:00     74.493004\n",
       "2019-09-13 03:00:00    107.165001\n",
       "2019-09-13 04:00:00    114.043991\n",
       "2019-09-13 05:00:00     39.010002\n",
       "2019-09-13 06:00:00     36.958000\n",
       "2019-09-13 07:00:00     68.350006\n",
       "2019-09-13 08:00:00     67.881004\n",
       "2019-09-13 09:00:00     96.738998\n",
       "2019-09-13 10:00:00     56.345005\n",
       "2019-09-13 11:00:00     19.947001\n",
       "2019-09-13 12:00:00     50.047009\n",
       "2019-09-13 13:00:00      1.284002\n",
       "2019-09-13 14:00:00    -48.785999\n",
       "Name: forecast_error_portfolio, dtype: float32"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_test_format[\"forecast_error_portfolio\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e0df3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.35000446],\n",
       "       [ 67.88100625],\n",
       "       [ 96.73899926],\n",
       "       [ 56.34500118],\n",
       "       [ 19.9469992 ],\n",
       "       [ 50.04700507],\n",
       "       [  1.2840028 ],\n",
       "       [-48.78599495],\n",
       "       [-65.74800631],\n",
       "       [-61.61499258],\n",
       "       [ -2.97500289],\n",
       "       [-22.35500249]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[t_to_check,:,:]\n",
    "# master_picture[t_to_check,lat_to_check,lon_to_check,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "de2fe534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_error_portfolio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.350004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.881006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.738999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.345001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.946999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>10.303001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>4.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>2.808001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>-6.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>-13.414999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2621 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      forecast_error_portfolio\n",
       "0                    68.350004\n",
       "1                    67.881006\n",
       "2                    96.738999\n",
       "3                    56.345001\n",
       "4                    19.946999\n",
       "...                        ...\n",
       "2616                 10.303001\n",
       "2617                  4.393000\n",
       "2618                  2.808001\n",
       "2619                 -6.562000\n",
       "2620                -13.414999\n",
       "\n",
       "[2621 rows x 1 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_val[:,t_plus,:], columns=prediction_col_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "89ba170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plus = 0\n",
    "head_offset = n_steps_in+t_plus\n",
    "tail_offset = -n_steps_in+t_plus+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0a30f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-09-13 07:00:00', '2019-09-13 08:00:00',\n",
       "               '2019-09-13 09:00:00', '2019-09-13 10:00:00',\n",
       "               '2019-09-13 11:00:00', '2019-09-13 12:00:00',\n",
       "               '2019-09-13 13:00:00', '2019-09-13 14:00:00',\n",
       "               '2019-09-13 15:00:00', '2019-09-13 16:00:00',\n",
       "               ...\n",
       "               '2019-12-31 02:00:00', '2019-12-31 03:00:00',\n",
       "               '2019-12-31 04:00:00', '2019-12-31 05:00:00',\n",
       "               '2019-12-31 06:00:00', '2019-12-31 07:00:00',\n",
       "               '2019-12-31 08:00:00', '2019-12-31 09:00:00',\n",
       "               '2019-12-31 10:00:00', '2019-12-31 11:00:00'],\n",
       "              dtype='datetime64[ns]', length=2621, freq=None)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_test_format.index[head_offset:tail_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "852ed788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for forecast error\n",
    "predictions_t = pd.DataFrame(predictions[:,t_plus,:], columns=prediction_col_schema)\n",
    "predictions_t.index = master_test_format.index[head_offset:tail_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ecc2863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = pd.DataFrame(y_val[:,t_plus,:], columns=prediction_col_schema)\n",
    "test_t.index = master_test_format.index[head_offset:tail_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e6057b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "forecast_t = pd.DataFrame(y_forecast[:,t_plus,:], columns=forecast_col_schema)\n",
    "forecast_t.index = master_test_format.index[head_offset:tail_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ca40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6a24e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_row = [\"portfolio\"]\n",
    "accuracy_row_mae = [\"portfolio\"]\n",
    "cols_to_report = [\"ufi\"] + [\"t_1\"]\n",
    "\n",
    "imbalance_report = pd.DataFrame()\n",
    "accuracy_report_mae = pd.DataFrame()\n",
    "rmse = RootMeanSquaredError()\n",
    "mae = MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "02e21584",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "prediction_forecast_error_metering = predictions_t.iloc[:,t].values\n",
    "test_set_forecast_error_metering = test_t.iloc[:,t].values\n",
    "baseline_forecast = forecast_t.iloc[:,t].values\n",
    "non_null_indices = np.argwhere(~np.isnan(baseline_forecast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "05c3389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_forecast = baseline_forecast[non_null_indices]\n",
    "prediction_forecast_error_metering = prediction_forecast_error_metering[non_null_indices]\n",
    "test_set_forecast_error_metering = test_set_forecast_error_metering[non_null_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4e9a4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_forecast = baseline_forecast + prediction_forecast_error_metering\n",
    "true_forecast = baseline_forecast + test_set_forecast_error_metering  # Should be equal to metering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "fcb45783",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_imbalance = np.sum(true_forecast) - np.sum(baseline_forecast)  # Should be equal to forecast_error_metering\n",
    "expected_imbalance = np.sum(true_forecast) - np.sum(expected_forecast)\n",
    "delta_imbalance = np.abs(baseline_imbalance) - np.abs(expected_imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d73840ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_ufi_t = mae(test_set_forecast_error_metering,prediction_forecast_error_metering).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f4f568ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_row = imbalance_row + [delta_imbalance]\n",
    "accuracy_row_mae = accuracy_row_mae + [mae_ufi_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d64e706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_report = imbalance_report.append(pd.Series(imbalance_row, index=cols_to_report), ignore_index=True)\n",
    "accuracy_report_mae = accuracy_report_mae.append(pd.Series(accuracy_row_mae, index=cols_to_report), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b4c2e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_1</th>\n",
       "      <th>ufi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5307.0</td>\n",
       "      <td>portfolio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t_1        ufi\n",
       "0  5307.0  portfolio"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b9d1f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_1</th>\n",
       "      <th>ufi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.254787</td>\n",
       "      <td>portfolio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         t_1        ufi\n",
       "0  44.254787  portfolio"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_report_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ef1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d75b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d763866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_outputs(master_test, predictions, n_steps_in, n_steps_out, portfolio_level=True):\n",
    "\n",
    "    print(\"Get forecasts from master test\")\n",
    "    _, y_forecast = split_sequences(master_test,n_steps_in,n_steps_out,target_col=\"forecast\")\n",
    "    # Get col schema for raw predictions\n",
    "    master_test_format = master_test.rename(columns={\"forecast_error_metering\":\"forecast_error\"}).copy()\n",
    "\n",
    "    if portfolio_level:\n",
    "        master_test_format[\"forecast_error_portfolio\"] = master_test_format[\"forecast_error\"].sum(axis=1)\n",
    "        prediction_col_schema = [\"forecast_error_portfolio\"]\n",
    "        forecast_col_schema = [\"forecast_portfolio\"]\n",
    "    else:\n",
    "        prediction_col_schema = master_test_format[\"forecast_error\"].columns\n",
    "        # Get col schema for forecasts to compute imbalances\n",
    "        forecasts_formatted = master_test.loc[:, (slice(\"forecast\"), slice(None), slice(None))]\n",
    "        forecast_col_schema = forecasts_formatted[\"forecast\"].columns\n",
    "\n",
    "    horizons = np.arange(0, n_steps_out)\n",
    "\n",
    "    predictions_formatted = pd.DataFrame()\n",
    "    test_set_formatted = pd.DataFrame()\n",
    "    forecast_set_formatted = pd.DataFrame()\n",
    "    predictions_t = pd.DataFrame()\n",
    "    print(\"Formatting predictions\")\n",
    "    for t_plus in horizons:\n",
    "        head_offset = n_steps_in+t_plus\n",
    "        tail_offset = -n_steps_out+t_plus+1\n",
    "        if tail_offset == 0:\n",
    "            tail_offset = None\n",
    "\n",
    "        # Predictions for forecast error\n",
    "        predictions_t = pd.DataFrame(predictions[:,t_plus,:], columns=prediction_col_schema)\n",
    "        predictions_t.index = master_test_format.index[head_offset:tail_offset]\n",
    "\n",
    "        if t_plus == horizons[0]:\n",
    "            predictions_formatted = predictions_t\n",
    "        else:\n",
    "            predictions_formatted = predictions_formatted.join(predictions_t, rsuffix=f\"_t{t_plus+1}\")\n",
    "\n",
    "        # True forecast error\n",
    "        test_t = pd.DataFrame(y_test[:,t_plus,:], columns=prediction_col_schema)\n",
    "        test_t.index = master_test_format.index[head_offset:tail_offset]\n",
    "\n",
    "        if t_plus == horizons[0]:\n",
    "            test_set_formatted = test_t\n",
    "        else:\n",
    "            test_set_formatted = test_set_formatted.join(test_t, rsuffix=f\"_t{t_plus+1}\")\n",
    "\n",
    "        # Forecast\n",
    "        forecast_t = pd.DataFrame(y_forecast[:,t_plus,:], columns=forecast_col_schema)\n",
    "        forecast_t.index = master_test_format.index[head_offset:tail_offset]\n",
    "\n",
    "        if t_plus == horizons[0]:\n",
    "            forecast_set_formatted = forecast_t\n",
    "        else:\n",
    "            forecast_set_formatted = forecast_set_formatted.join(forecast_t, rsuffix=f\"_t{t_plus+1}\")\n",
    "\n",
    "    return predictions_formatted, test_set_formatted, forecast_set_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "346b6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions_formatted, test_set_formatted, forecast_set_formatted, n_steps_out):\n",
    "   \n",
    "    horizons = np.arange(0, n_steps_out)\n",
    "    cols_to_report = [\"ufi\"] + list(horizons)\n",
    "\n",
    "    imbalance_row = [\"portfolio\"]\n",
    "    accuracy_row_mae = [\"portfolio\"]\n",
    "\n",
    "    imbalance_report = pd.DataFrame()\n",
    "    accuracy_report_mae = pd.DataFrame()\n",
    "    rmse = RootMeanSquaredError()\n",
    "    mae = MeanAbsoluteError()\n",
    "\n",
    "    for t in horizons:\n",
    "\n",
    "        if t == 0:\n",
    "            suffix = \"\"\n",
    "        else:\n",
    "            suffix = f\"_t{t+1}\"\n",
    "\n",
    "        prediction_forecast_error_metering = predictions_formatted.iloc[:,t].values\n",
    "        test_set_forecast_error_metering = test_set_formatted.iloc[:,t].values\n",
    "\n",
    "        # Due to the way we query the forecasts there are gaps which result in nan values\n",
    "        baseline_forecast = forecast_set_formatted.iloc[:,t].values\n",
    "        non_null_indices = np.argwhere(~np.isnan(baseline_forecast))\n",
    "        # Filter out data without target info\n",
    "        baseline_forecast = baseline_forecast[non_null_indices]\n",
    "        prediction_forecast_error_metering = prediction_forecast_error_metering[non_null_indices]\n",
    "        test_set_forecast_error_metering = test_set_forecast_error_metering[non_null_indices]\n",
    "\n",
    "        expected_forecast = baseline_forecast + prediction_forecast_error_metering\n",
    "        true_forecast = baseline_forecast + test_set_forecast_error_metering  # Should be equal to metering\n",
    "\n",
    "        baseline_imbalance = np.sum(true_forecast) - np.sum(baseline_forecast)  # Should be equal to forecast_error_metering\n",
    "        expected_imbalance = np.sum(true_forecast) - np.sum(expected_forecast)\n",
    "        delta_imbalance = np.abs(baseline_imbalance) - np.abs(expected_imbalance)\n",
    "\n",
    "        mae_ufi_t = mae(test_set_forecast_error_metering,prediction_forecast_error_metering).numpy()\n",
    "    \n",
    "        imbalance_row = imbalance_row + [delta_imbalance]\n",
    "        accuracy_row_mae = accuracy_row_mae + [mae_ufi_t]\n",
    "\n",
    "    imbalance_report = imbalance_report.append(pd.Series(imbalance_row, index=cols_to_report), ignore_index=True)\n",
    "    accuracy_report_mae = accuracy_report_mae.append(pd.Series(accuracy_row_mae, index=cols_to_report), ignore_index=True)\n",
    "    \n",
    "    return imbalance_report, accuracy_report_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf2bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729866b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30701bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91556a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb08d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098770b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc23162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4d29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9c706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:windTFM]",
   "language": "python",
   "name": "conda-env-windTFM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
